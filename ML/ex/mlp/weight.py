'''
The Computer class between 2 nodes
'''


import numpy as np



class Weight:
    '''
    The Weight Matrix

    Variables:
        shape: (the number of previous nodes, 
            the number of next nodes)
    
        weight: the weight matrix;
            row is the number of previous nodes
            col is the number of next nodes

        bias: the bias victor

        input_data: input matrix with bias '1' col vector
            
        output_data: the matrix generated by weight

        err: the error matrix

        der: the derivative matrix

    Functions:
        __init__: initialize the class

        sigmoid: sigmoid function
            the activation function

        get_output: generate output matrix
            according to input
            the forward propagation

        
        get_derivative: allocate loss, compute grad
            according to next weight
            the backward propagation

        update: update weights
            according the input matrix, der matrix
            the gradient decient


    '''



    def __init__(self, prev_nodes, next_nodes)-> None:
        self.shape = (prev_nodes, next_nodes)
        self.bias = np.random.rand(1,next_nodes)
        self.weight = np.random.rand(self.shape[0],self.shape[1])
        self.input_data = None
        self.output_data = None
        self.der = None
        self.err = None

    def sigmoid(self, z) -> np.ndarray:
        """
        chose sigmoid function as activate function
        """
        return 1 / (1 + np.exp(-z))

    def get_output(self, input_data) -> np.ndarray:
        """
        get output and store it in the object

        Parameters:
        input:np.ndarray

        Returns:
        np.ndarray
        """
        if not isinstance(input_data, np.ndarray):
            return None
        self.input_data = np.array(input_data)
        self.output_data = np.matmul(self.input_data, self.weight) + self.bias
        self.output_data = self.sigmoid(self.output_data)
        return self.output_data

    def get_derivative(self, next_weight, result = None,is_end = False)->np.ndarray:
        """
        next_weight: next layer loss

        result: finally really result

        is_end: flag to verify if the layer is the last layer

        the derivativa will be stored in der
        """
        if is_end:
            self.err = self.output_data - result
        else:
            self.err = np.matmul(next_weight.der, next_weight.weight.T)
        self.der = np.multiply(self.err, self.output_data*(1 - self.output_data))
        return self.der


    def update(self, rate, regularization_rate) -> None:
        """
        update the weight
        """
        grad = self.input_data.T.dot(self.der)
        if not(regularization_rate > 1 or regularization_rate < 0):
            grad += regularization_rate * self.weight
        self.weight -= grad * rate
        self.bias -= rate * np.sum(self.der,axis=0)
