'''
The Computer class between 2 nodes
'''


import numpy as np



class Weight:
    '''
    The Weight Matrix

    Variables:
        shape: (the number of previous nodes, 
            the number of next nodes)
    
        weight: the weight matrix;
            row is the number of previous nodes
            col is the number of next nodes

        input: input matrix with bias '1' col vector
            
        output: the matrix generated by weight

        err: the error matrix

        der: the derivative matrix

    Functions:
        __init__: initialize the class

        sigmoid: sigmoid function
            the activation function

        get_output: generate output matrix
            according to input
            the forward propagation

        
        get_derivative: allocate loss, compute grad
            according to loss
            the backward propagation

        update: update weights
            according the input matrix, der matrix
            the gradient decient


    '''



    def __init__(self, prev_nodes, next_nodes)-> None:
        self.shape = (prev_nodes+1, next_nodes)
        self.weight = np.ones(self.shape)
        self.input = None
        self.output = None
        self.der = None
        self.err = None

    def sigmoid(self, z) -> np.ndarray:
        """
        chose sigmoid function as activate function
        """
        return 1 / (1 + np.exp(-z))

    def get_output(self, input_data) -> np.ndarray:
        """
        get output and store it in the object

        Parameters:
        input:np.ndarray

        Returns:
        np.ndarray
        """
        if not isinstance(input_data, np.ndarray):
            return None
        bias = np.ones(input.shape[0])
        self.input = np.hstack((bias, input_data))
        self.output = np.matmul(self.input, self.weight)
        self.output = self.sigmoid(self.output)
        return self.output

    def get_derivative(self, next_loss, result = None,is_end = False)->np.ndarray:
        """
        next_loss: next layer loss

        result: finally really result

        is_end: flag to verify if the layer is the last layer
        """
        if is_end:
            self.err = result - self.output
        else:
            self.err = np.matmul(next_loss, self.weight)
        self.der = np.multiply(self.err, self.output*(1 - self.output))
        return self.der


    def update(self, rate) -> None:
        """
        update the weight
        """
        self.weight += self.input.T.dot(self.der)*rate
